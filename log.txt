2	1
8	1
7	1
8	1
2	1
5	1
0	1
9	1
8	1
1	1
9	1
4	1
5	1
5	1
8	1
1	1
5	1
3	1
0	1
1	1
7	1
5	1
6	1
7	1
1	1
7	1
3	1
6	1
1	1
3	1
3	1
2	1
0	1
6	1
9	1
8	1
1	1
1	1
2	1
5	1
0	1
9	1
9	1
6	1
1	1
8	1
1	1
8	1
8	1
1	1
5	1
9	1
3	1
0	1
4	1
1	1
6	1
9	1
0	1
3	1
5	1
1	1
5	1
9	1
8	1
8	1
8	1
8	1
5	1
1	1
9	1
3	1
4	1
5	1
8	1
0	1
7	1
2	1
7	1
3	1
8	1
6	1
6	1
7	1
3	1
8	1
5	1
8	1
9	1
4	1
2	1
2	1
8	1
7	1
9	1
2	1
2	1
8	1
4	1
9	1
9	1
8	1
9	1
2	1
0	1
8	1
6	1
8	1
0	1
5	1
8	1
2	1
5	1
7	1
4	1
9	1
2	1
7	1
9	1
6	1
1	1
0	1
4	1
8	1
4	1
1	1
9	1
8	1
4	1
4	1
4	1
3	1
6	1
3	1
4	1
6	1
3	1
2	1
4	1
4	1
9	1
6	1
8	1
4	1
8	1
7	1
5	1
6	1
0	1
2	1
3	1
3	1
6	1
2	1
4	1
8	1
2	1
7	1
0	1
4	1
1	1
9	1
7	1
8	1
6	1
2	1
3	1
2	1
0	1
9	1
0	1
0	1
2	1
1	1
6	1
0	1
9	1
9	1
0	1
2	1
3	1
5	1
3	1
0	1
4	1
3	1
6	1
9	1
9	1
4	1
1	1
8	1
4	1
9	1
1	1
4	1
6	1
3	1
1	1
4	1
0	1
9	1
3	1
4	1
3	1
1	1
7	1
3	1
8	1
1	1
4	1
3	1
6	1
4	1
0	1
5	1
4	1
6	1
2	1
5	1
3	1
1	1
5	1
2	1
0	1
9	1
6	1
1	1
8	1
3	1
6	1
9	1
0	1
8	1
8	1
8	1
7	1
0	1
7	1
0	1
1	1
6	1
7	1
6	1
8	1
3	1
9	1
6	1
4	1
2	1
4	1
3	1
7	1
8	1
1	1
4	1
0	1
5	1
9	1
2	1
7	1
1	1
4	1
5	1
6	1
3	1
5	1
4	1
9	1
0	1
6	1
1	1
3	1
0	1
3	1
1	1
0	1
7	1
2	1
0	1
8	1
5	1
1	1
0	1
3	1
8	1
3	1
7	1
5	1
0	1
5	1
1	1
[cloudera@quickstart Desktop]$ chmod +x reducer.py
[cloudera@quickstart Desktop]$ cat text.rtf | python mapper.py | sort -k1,1 | python reducer.py
  File "reducer.py", line 26
    if current_digit = digit:
                     ^
SyntaxError: invalid syntax
[cloudera@quickstart Desktop]$ cat text.rtf | python mapper.py | sort -k1,1 | python reducer.py
Traceback (most recent call last):
  File "reducer.py", line 18, in <module>
    if cuurent_digit == digit:
NameError: name 'cuurent_digit' is not defined
[cloudera@quickstart Desktop]$ cat text.rtf | python mapper.py | sort -k1,1 | python reducer.py
0	95
1	89
2	96
3	106
4	96
5	78
6	93
7	92
8	98
9	108
[cloudera@quickstart Desktop]$ hdfs dfs -put text.rtf /user/cloudera
put: Cannot create file/user/cloudera/text.rtf._COPYING_. Name node is in safe mode.
[cloudera@quickstart Desktop]$ hdfs dfsadmin -safemode leave
Safe mode is OFF
[cloudera@quickstart Desktop]$ hdfs dfs -put text.rtf /user/cloudera
[cloudera@quickstart Desktop]$ ls /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.13.0.jar 
/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.13.0.jar
[cloudera@quickstart Desktop]$ ^C
[cloudera@quickstart Desktop]$ hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.13.0.jar -Dmapred.reduce.tasks=1 -file /home/cloudera/Desktop/mapper.py /home/cloudera/Desktop/reducer.py -mapper "python mapper.py" -reduce "python reducer.py" -input /user/cloudera/text.rtf -output /home/cloudera/Desktop
19/03/09 18:27:02 ERROR streaming.StreamJob: Unrecognized option: -reduce
Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar [options]
Options:
  -input          <path> DFS input file(s) for the Map step.
  -output         <path> DFS output directory for the Reduce step.
  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.
  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.
  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.
  -file           <file> Optional. File/dir to be shipped in the Job jar file.
                  Deprecated. Use generic option "-files" instead.
  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>
                  Optional. The input format class.
  -outputformat   <TextOutputFormat(default)|JavaClassName>
                  Optional. The output format class.
  -partitioner    <JavaClassName>  Optional. The partitioner class.
  -numReduceTasks <num> Optional. Number of reduce tasks.
  -inputreader    <spec> Optional. Input recordreader spec.
  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.
  -mapdebug       <cmd> Optional. To run this script when a map task fails.
  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.
  -io             <identifier> Optional. Format to use for input to and output
                  from mapper/reducer commands
  -lazyOutput     Optional. Lazily create Output.
  -background     Optional. Submit the job and don't wait till it completes.
  -verbose        Optional. Print verbose output.
  -info           Optional. Print detailed usage.
  -help           Optional. Print help message.

Generic options supported are
-conf <configuration file>     specify an application configuration file
-D <property=value>            use value for given property
-fs <local|namenode:port>      specify a namenode
-jt <local|resourcemanager:port>    specify a ResourceManager
-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster
-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]


For more details about these options:
Use $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info
[cloudera@quickstart Desktop]$ hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.13.0.jar -file /home/cloudera/Desktop/mapper.py /home/cloudera/Desktop/reducer.py -mapper "python mapper.py" -reducer "python reducer.py" -input /user/cloudera/text.rtf -output /home/cloudera/Desktop
19/03/09 18:30:09 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.home/cloudera/Desktop/reducer.py -mapper "python mapper.py" -reducer "ppackageJobJar: [/home/cloudera/Desktop/mapper.py, /home/cloudera/Desktop/reducer.py] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.13.0.jar] /tmp/streamjob7069516653115222214.jar tmpDir=null
19/03/09 18:30:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
19/03/09 18:30:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
19/03/09 18:30:18 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
19/03/09 18:30:18 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
19/03/09 18:30:18 INFO mapred.FileInputFormat: Total input paths to process : 1
19/03/09 18:30:18 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
19/03/09 18:30:18 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
19/03/09 18:30:18 INFO mapreduce.JobSubmitter: number of splits:2
19/03/09 18:30:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1552176796202_0001
19/03/09 18:30:21 INFO impl.YarnClientImpl: Submitted application application_1552176796202_0001
19/03/09 18:30:22 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1552176796202_0001/
19/03/09 18:30:22 INFO mapreduce.Job: Running job: job_1552176796202_0001
19/03/09 18:30:47 INFO mapreduce.Job: Job job_1552176796202_0001 running in uber mode : false
19/03/09 18:30:47 INFO mapreduce.Job:  map 0% reduce 0%
19/03/09 18:31:31 INFO mapreduce.Job:  map 67% reduce 0%
19/03/09 18:31:32 INFO mapreduce.Job:  map 83% reduce 0%
19/03/09 18:31:33 INFO mapreduce.Job:  map 100% reduce 0%
19/03/09 18:32:01 INFO mapreduce.Job:  map 100% reduce 100%
19/03/09 18:32:02 INFO mapreduce.Job: Job job_1552176796202_0001 completed successfully
19/03/09 18:32:02 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=5712
		FILE: Number of bytes written=453059
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1642
		HDFS: Number of bytes written=52
		HDFS: Number of read operations=9
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=2
		Launched reduce tasks=1
		Data-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=85537
		Total time spent by all reduces in occupied slots (ms)=25048
		Total time spent by all map tasks (ms)=85537
		Total time spent by all reduce tasks (ms)=25048
		Total vcore-milliseconds taken by all map tasks=85537
		Total vcore-milliseconds taken by all reduce tasks=25048
		Total megabyte-milliseconds taken by all map tasks=87589888
		Total megabyte-milliseconds taken by all reduce tasks=25649152
	Map-Reduce Framework
		Map input records=1
		Map output records=951
		Map output bytes=3804
		Map output materialized bytes=5718
		Input split bytes=212
		Combine input records=0
		Combine output records=0
		Reduce input groups=10
		Reduce shuffle bytes=5718
		Reduce input records=951
		Reduce output records=10
		Spilled Records=1902
		Shuffled Maps =2
		Failed Shuffles=0
		Merged Map outputs=2
		GC time elapsed (ms)=2791
		CPU time spent (ms)=6440
		Physical memory (bytes) snapshot=553709568
		Virtual memory (bytes) snapshot=4524883968
		Total committed heap usage (bytes)=391979008
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1430
	File Output Format Counters 
		Bytes Written=52
19/03/09 18:32:02 INFO streaming.StreamJob: Output directory: /home/cloudera/Desktop
[cloudera@quickstart Desktop]$ hdfs dfs -ls /user/cloudera/Desktop
ls: `/user/cloudera/Desktop': No such file or directory
[cloudera@quickstart Desktop]$ hdfs dfs -ls /home/cloudera/Desktop
Found 2 items
-rw-r--r--   1 cloudera supergroup          0 2019-03-09 18:32 /home/cloudera/Desktop/_SUCCESS
-rw-r--r--   1 cloudera supergroup         52 2019-03-09 18:31 /home/cloudera/Desktop/part-00000
[cloudera@quickstart Desktop]$ hdfs dfs -cat /home/cloudera/Desktop/part*
0	95
1	89
2	96
3	106
4	96
5	78
6	93
7	92
8	98
9	108
[cloudera@quickstart Desktop]$ hdfs dfs -cat /home/cloudera/Desktop/part* > result.txt
[cloudera@quickstart Desktop]$ 
